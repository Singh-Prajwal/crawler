# E-Commerce Product URL Crawler ğŸ›’

This is a fast, scalable, and reliable asynchronous web crawler designed to extract product page URLs from multiple e-commerce websites. It supports different domain structures and handles edge cases to ensure high performance and accuracy.

---

## ğŸ“¦ Features

- âœ… Discovers product URLs using domain-specific patterns
- âš™ï¸ Handles asynchronous crawling with controlled concurrency
- ğŸ§  Smart internal link following (avoids external links)
- ğŸš€ Designed for large websites with deep navigation
- ğŸ“€ Outputs clean JSON with all product and visited URLs
- â™»ï¸ Retry logic for failed requests
- ğŸ§± Handles broken HTML, timeouts, and inconsistent structures

---

## ğŸ“ Project Structure

```
crawler/
â”œâ”€â”€ config.py         # URL patterns per domain
â”œâ”€â”€ fetcher.py        # Handles HTTP requests with retries
â”œâ”€â”€ parser.py         # Extracts internal links from HTML
â”œâ”€â”€ main.py           # Core crawler logic
â””â”€â”€ output/
    â”œâ”€â”€ product_urls.json   # Collected product links
    â””â”€â”€ visited_urls.json   # All visited URLs
```

---

## ğŸš€ How to Run

1. **Clone the repo**

```bash
git clone https://github.com/Singh-Prajwal/crawler.git
cd ecom-crawler
```

2. **(Optional) Add your domains**

By default, `main.py` includes 4 example domains. You can edit them in the `domains` list.

```python
domains = [
    "https://www.virgio.com/",
    "https://www.tatacliq.com/",
    "https://www.nykaafashion.com/",
    "https://www.westside.com/"
]
```

3. **Install requirements**

```bash
pip install -r requirements.txt
```

4. **Run the crawler**

```bash
python crawler/main.py
```

---

## ğŸ“¤ Output

- `product_urls.json`: All discovered product URLs grouped by domain and visited by the crawler.

---

## ğŸ› ï¸ Customizing

You can update `config.py` to add domain-specific URL matching patterns:

```python
DOMAIN_PATTERNS = {
    "example.com": [r"/product/.*", r"/item/.*"]
}
```

These patterns help the crawler know which URLs are likely to be product pages.

---

## ğŸ“Œ Notes

- The crawler respects internal links only (it won't crawl external sites).
- It uses a depth limit (`MAX_DEPTH`) to avoid infinite loops or unnecessary crawling.
- Error handling and retry logic are built in to make it resilient against flaky pages or failed requests.

---

## ğŸ‘¤ Author

Built with care by Prajwal Singh.  